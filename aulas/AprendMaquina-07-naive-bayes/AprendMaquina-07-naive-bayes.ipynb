{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Aprendizagem Probabilística -- Classificação Usando Naive Bayes\"\n",
        "subtitle: \"ESTAT0016 -- Tópicos Especiais em Estatística (Introdução à Apredizagem de Máquina)\"\n",
        "author: \"Prof. Dr. Sadraque E.F. Lucena\"\n",
        "logo: images/ufs_horizontal_positiva.png\n",
        "lang: pt\n",
        "format: \n",
        "  revealjs:\n",
        "    theme: slides.scss\n",
        "    multiplex: true    # permite visualização simultânea por várias pessoas\n",
        "    transition: fade\n",
        "    slide-number: true\n",
        "    auto-stretch: false\n",
        "    menu: false\n",
        "    self-contained: false\n",
        "    width: 1600\n",
        "    height: 900\n",
        "    css: slides.css\n",
        "    echo: false\n",
        "    code-link: true\n",
        "editor: source\n",
        "---"
      ],
      "id": "328c6635"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Introdução\n",
        "\n",
        "- O algoritmo Naive Bayes (Bayes ingênuo) é parte dos chamados *métodos bayesianos*.\n",
        "- A partir dos dados de entrada, o classificadores baseados em métodos bayesianos usam dados de treino para calcular a probabilidade do resultado pertencer a uma classe.\n",
        "- Classificadores bayesianos podem ser usados para:\n",
        "\n",
        "  - Classificação de texto, como detecção de e-mail de spam;\n",
        "  - Detecção de intrusos ou anomalias em uma rede de computadores;\n",
        "  - Diagnóstico médico dado um conjunto de sintomas observados.\n",
        "\n",
        "- Classificadores bayesianos são ideais para problemas que requerem a consideração simultânea de vários atributos.\n",
        "- Mesmo atributos com efeitos relativamente pequenos podem ter um impacto significativo quando combinados em um modelo bayesiano.\n",
        "\n",
        "\n",
        "## Terminologia\n",
        "\n",
        "- Classe $w_i$ (variável aleatória): categorias possíveis que estamos tentando prever.\n",
        "- Probabilidade **a priori**:\n",
        "  - Conhecimento prévio sobre o problema\n",
        "  - Ou seja, conhecimento sobre a aparição de exemplos das classes do problema\n",
        "- Probabilidade **a posteriori**:\n",
        "  - Probabilidade de pertencer a uma classe após consideração dos dados de entrada\n",
        "\n",
        "\n",
        "## Base: Teorema de Bayes\n",
        "\n",
        "- Supomos que conhecemos a probabilidade de um evento $A$, denotada por $P(A)$.\n",
        "- Ao obter informações adicionais, podemos ajustar a probabilidade do evento $A$ ocorrer.\n",
        "  - Denotamos esse ajuste como $P(A|B)$, que representa a probabilidade de $A$ dado que o evento $B$ ocorreu.\n",
        "- O Teorema de Bayes é fundamental nesse contexto.\n",
        "  - Ele recalcula as probabilidades de uma instância pertencer a uma classe após a obtenção de novas evidências.\n",
        "\n",
        "\n",
        "## Base: Teorema de Bayes\n",
        "\n",
        "- Se $A$ e $B$ dois eventos, podemos caluclar a probabilidade de $A$ dado $B$ uasndo o teorema de Bayes:\n",
        "$$\n",
        "  P(A|B) = \\frac{P(A\\cap B)}{P(B)} = \\frac{P(B|A)P(A)}{P(B)}\n",
        "$$\n",
        "\n",
        "- $P(A|B)$: probabilidade **a posteriori** (*posterior probability*), probabilidade de $A$ após observar $B$\n",
        "- $P(A)$: probabildiade **a priori** (*prior probability*), conhecimento inicial sobre $A$\n",
        "- $P(B|A)$: verossimilhança (*likelihood*)\n",
        "- $P(B)$: probabilidade marginal (*marginal likelihood*)\n",
        "\n",
        "## Exemplo 7.1\n",
        "\n",
        "Calcule a probabilidade de um email ser *spam* a partir da ocorrência do termo \"viagra\" no texto. A tabela abaixo apresenta os dados.\n",
        "\n",
        "+------------+-----------+-------+\n",
        "| $~$        | viagra    | $~$   |\n",
        "|            +-----+-----+       |\n",
        "| Frequência | Sim | Não | Total |\n",
        "+:===========+:===:+:===:+:=====:+\n",
        "| *spam*     | 4   | 16  | 20    |\n",
        "+------------+-----+-----+-------+\n",
        "| não *spam* | 1   | 79  | 80    |\n",
        "+------------+-----+-----+-------+\n",
        "| Total      | 5   | 95  | 100   |\n",
        "+------------+-----+-----+-------+\n",
        "\n",
        "## O classificador Naive Bayes\n",
        "\n",
        "- O método é denominado \"ingênuo\" (*naive*) devido à suposição de que todos os atributos são igualmente importantes e independentes, o que raramente é verdadeiro na prática.\n",
        "- Exemplo: em emails a a relevância de atributos varia.\n",
        "  - O remetente tem maior peso que o texto.\n",
        "  - Algumas palavras não são independentes (ex: \"viagra\" sugere \"prescrição\" ou \"remédio\").\n",
        "\n",
        "- **Observação:** Embora as suposições do método Naive Bayes não corresponda completamente à realidade, ele ainda assim apresenta um desempenho razoável.\n",
        "- Naive Bayes frequentemente se destaca como uma escolha razoável para tarefas de aprendizado de classificação, especialmente em conjuntos de dados de treino menores.\n",
        "\n",
        "## Exemplo 7.2\n",
        "\n",
        "Vamos extender nosso filtro de spam adicionando mais termos a serem monitorados: \"viagra\", \"dinheiro\", \"compra\" e \"descadastrar\":\n",
        "\n",
        "+-----------------+-------------------+--------------------+-------------------+--------------------+--------+\n",
        "| $~$             | viagra            | dinheiro           | compra            | descadastrar       | $~$    |\n",
        "+                 +---------+---------+----------+---------+---------+---------+----------+---------+        +\n",
        "| Verossimilhança | Sim     | Não     | Sim      | Não     | Sim     | Não     | Sim      | Não     | Total   |\n",
        "+:================+:=======:+:=======:+:========:+:=======:+:=======:+:=======:+:========:+:=======:+:======:+\n",
        "| *spam*          | 4/20    | 16/20   | 10/20    | 10/20   | 0/20    | 20/20   | 12/20    | 8/20    | 20     |\n",
        "+-----------------+---------+---------+----------+---------+---------+---------+----------+---------+--------+\n",
        "| não *spam*      | 1/80    | 79/80   | 14/80    | 66/80   | 8/80    | 71/80   | 23/80    | 57/80   | 80     |\n",
        "+-----------------+---------+---------+----------+---------+---------+---------+----------+---------+--------+\n",
        "| Total           | 5/100   | 95/100  | 24/100   | 76/100  | 8/100   | 91/100  | 35/100   | 65/100  | 100    |\n",
        "+-----------------+---------+---------+----------+---------+---------+---------+----------+---------+--------+\n",
        "\n",
        "\n",
        "```{latex}\n",
        "\\begin{table}[h]\n",
        "\\centering\n",
        "\\begin{tabular}{|c|c|c|}\n",
        "\\hline\n",
        "Célula 1 & \\multicolumn{2}{c|}{Célula 2-3} \\\\ \\hline\n",
        "Texto & \\multicolumn{2}{c|}{\\centering Mesclada} \\\\ \\hline\n",
        "& Outra célula & Mais \\\\ \\hline\n",
        "Texto & \\multicolumn{2}{c|}{Outra Mesclada} \\\\ \\hline\n",
        "\\end{tabular}\n",
        "\\caption{Exemplo de Tabela com Mesclagem de Colunas}\n",
        "\\end{table}\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "## Vantagens e desvantagens\n",
        "\n",
        "::: {.callout-note}\n",
        "## Vantagens\n",
        "\n",
        "- Simples e efetivo.\n",
        "- Não faz suposições sobre a distribuição dos dados.\n",
        "- Fase de treinamento rápida.\n",
        ":::\n",
        "\n",
        "::: {.callout-important}\n",
        "## Desvantagens\n",
        "\n",
        "- Não produz um modelo, limitando a capacidade de entender como as características se relacionam com a classe.\n",
        "- Requer a seleção de um $k$ apropriado.\n",
        "- Fase de treinamento rápida.\n",
        "- Fase de classificação lenta.\n",
        "- Características nominais e dados ausentes exigem processamento adicional.\n",
        ":::\n",
        "\n",
        "## Encontrando os vizinhos mais próximos\n",
        "\n",
        "- Para encontrar os vizinhos mais pŕoximos de uma instância é preciso calcular a distância entre as instâncias.\n",
        "- Tradicionalmente, o algoritmo k-NN usa a **distância euclidiana**.\n",
        "- Seja $p$ e $q$ duas instâncias com $n$ atritubos. Então a distância euclidiana entre $p$ e $q$ é dada por\n",
        "$$\n",
        "  dist(p,q) = \\sqrt{(p_1-q_1)^2 +(p_2-q_2)^2 + \\cdots + (p_n-q_n)^2}\n",
        "$$\n",
        "em que $p_i$ e $q_i$, $i=1,\\ldots,n$ representam os atributos associados às instâncias $p$ e $q$, respectivamente.\n",
        "\n",
        "\n",
        "## Preparando os dados\n",
        "\n",
        "- **Observação:** antes do cálculo da distância euclidiana devemos normalizar os atributos, pois atributos com valores mais elevados tendem a ter um impacto desproporcional no cálculo de distância.\n",
        "- Para o k-NN podemos usar a *normalização min-max*:\n",
        "$$\n",
        "x_{novo} = \\frac{x - min(X)}{max(X)-min(X)}\n",
        "$$\n",
        "- ou a transformação *z-score*:\n",
        "$$\n",
        "x_{novo} = \\frac{x - média(X)}{DesvPad(X)}.\n",
        "$$\n",
        "\n",
        "## Preparando os dados\n",
        "\n",
        "- Se o atributo é do tipo nomial, devemos transformá-lo em uma variável *dummy*. Por exemplo:\n",
        "$$\n",
        "  homem = \\begin{cases}\n",
        "    1 & \\text{se } x = \\text{ homem}\\\\\n",
        "    0 & \\text{caso contrário}.\n",
        "  \\end{cases}\n",
        "$$\n",
        "- Se a variável tem mais de duas categorias, digamos $n$, devem ser criadas $n-1$ variáveis *dummies*.\n",
        "- Exemplo: se o atributo *temperatura* possui as categorias *quente*, *médio* e *frio*, devem ser criados:\n",
        "\n",
        ":::: {.columns}\n",
        "::: {.column width=\"50%\"}\n",
        "$$\n",
        "quente = \\begin{cases}\n",
        "  1 & \\text{se } x = \\text{ quente}\\\\\n",
        "  0 & \\text{caso contrário}\n",
        "\\end{cases}\n",
        "$$\n",
        ":::\n",
        "::: {.column width=\"50%\"}\n",
        "$$\n",
        "médio = \\begin{cases}\n",
        "  1 & \\text{se } x = \\text{ médio}\\\\\n",
        "  0 & \\text{caso contrário}\n",
        "\\end{cases}\n",
        "$$\n",
        ":::\n",
        "::::\n",
        "\n",
        "- Como uma dummy possui apenas os valores 0 e 1, os valores caem na mesma escala da transformação min-max.\n",
        "\n",
        "## Exemplo 6.1\n",
        "Considere os dados de treinamento:\n",
        "\n",
        "Paciente | Idade | Colesterol | Doença cardíaca | Paciente | Idade | Colesterol | Doença cardíaca\n",
        ":-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:\n",
        "1 | 45 | 297 | FALSO | 6 | 48 | 256 | VERDADEIRO\n",
        "2 | 41 | 172 | VERDADEIRO | 7 | 49 | 212 | VERDADEIRO\n",
        "3 | 46 | 202 | VERDADEIRO | 8 | 41 | 289 | VERDADEIRO\n",
        "4 | 48 | 193 | VERDADEIRO | 9 | 49 | 271 | FALSO\n",
        "5 | 46 | 243 | FALSO | 10 | 43 | 315 | FALSO\n",
        "\n",
        "Calcule a distância euclidiana para um novo paciente com 45 anos e colesterol de 225 usando a normalização min-max. Ordende os dados de treino da menor distância para a maior ditância do novo paciente.\n",
        "\n",
        "# Determinando $k$ apropriado\n",
        "\n",
        "- A decisão de quantos vizinhos usar para o k-NN determina o quão bem o modelo generalizará dados futuros.\n",
        "- O equilíbrio entre *overfitting* e *underfitting* aos dados de treinamento é um problema conhecido como o *tradeoff entre viés e variância*.\n",
        "- Escolher um $k$ pequeno pode tornar o modelo muito sensível à ruído nos dados, levando ao *overfitting*.\n",
        "- Um valor grande de $k$ pode enviesar o aprendizado, correndo o risco de ignorar padrões pequenos, porém importantes.\n",
        "\n",
        "\n",
        "# Determinando $k$ apropriado\n",
        "\n",
        "- O valor escolhido para $k$ deve sempre ser ímpar, para evitar empates na hora de classificar.\n",
        "- Uma forma de determinar $k$ é testar diversos valores de $k$ com os dados de teste e escolher aquele com melhor performance de classificação.\n",
        "\n",
        "\n",
        "## Por que o algoritmo k-NN é preguiçoso?\n",
        "\n",
        "- Algoritmos de classificação baseados em métodos de vizinho mais próximo são considerados algoritmos de aprendizado preguiçoso (*lazy learning*).\n",
        "- Um aprendiz preguiçoso não está realmente aprendendo nada; ele apenas armazena os dados de treinamento sem qualquer abstração.\n",
        "- O aprendizado preguiçoso também é conhecido como aprendizado baseado em instâncias ou aprendizado por repetição.\n",
        "\n",
        "\n",
        "## E a Regressão k-NN?\n",
        "\n",
        "- Em problemas de regressão, como a variável resposta é numérica, a estimativa é dada pela média dos $k$ vizinhos mais próximos. Duas formas são possíveis:\n",
        "  - Usar a média aritmética ou\n",
        "  - Usar a média ponderada pela distância entre as instâncias (preferível).\n",
        "\n",
        "- A média ponderada pelas distâncias é dada por\n",
        "$$\n",
        "  \\widehat{y}_i = \\frac{\\sum\\limits_{t=0}^k y_i\\, p_i}{\\sum\\limits_{t=0}^k p_i},\n",
        "$$\n",
        "\n",
        "em que:\n",
        "- $y_i$ é o valor da variável resposta para a instância $i$;\n",
        "- $p_i$ é o peso dado pelo inverso da distância entre a nova instaância e a instância de treino.\n",
        "\n",
        "## Escolha de $k$ na Regressão k-NN\n",
        "\n",
        "- O valor de $k$ é escolhido como aquele que produz menor erro. Algumas métricas que podem ser usadas para quantificar o erro são:\n",
        "\n",
        "  - Erro médio absoluto (MAE):  $~~~~~MAE(y,\\widehat{y}) = \\frac{1}{n}\\sum\\limits_{i=1}^n|y_i-\\widehat{y}_i|$\n",
        "  - Erro quadrático médio (MSE):$~~~~~MSE(y,\\widehat{y}) = \\frac{1}{n}\\sum\\limits_{i=1}^n (y_i-\\widehat{y}_i)^2$\n",
        "  - Raiz do erro quadrático médio (RMSE):$~~~~~RMSE(y,\\widehat{y}) = \\sqrt{\\frac{1}{n}\\sum\\limits_{i=1}^n (y_i-\\widehat{y}_i)^2}$\n",
        "\n",
        "# FIM {style=\"text-align: center;\"}\n",
        "![](images/giphy.gif){fig-align=\"center\"}"
      ],
      "id": "c9a52b6c"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}